{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PLEASE GO THE FOLLOWING LINK TO RUN THE CODE https://colab.research.google.com/drive/1u7PHpb2CAS7DMYpy9sBmvOhdYCzVwZuC?usp=sharing"
      ],
      "metadata": {
        "id": "65FXuZHyzKyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"2D_Matrix.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1u7PHpb2CAS7DMYpy9sBmvOhdYCzVwZuC\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VPsR6QGxzKKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "XhrcelZSzUsJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START = (7,0)\n",
        "BOARD_ROWS = 8\n",
        "BOARD_COLS = 8\n",
        "WIN_STATE  = (0,7)\n",
        "Zombie = (0,6)\n",
        "Wolf = (3,5)"
      ],
      "metadata": {
        "id": "OZm5i-mo1c9z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State():\n",
        "  \"\"\"\n",
        "    This is a State class which controls all the cells state in the 2d grid.\n",
        "    In our 2d board depending upon the action by the agent the current state\n",
        "    can be changed or it can be a episode (win or lose).\n",
        "\n",
        "    :param START: Players start value anywhere in the board\n",
        "    :type START: tuple, Mandatory\n",
        "  \"\"\"\n",
        "  def __init__(self, state = START):\n",
        "    \"\"\"\n",
        "    Constructor Method\n",
        "    \"\"\"\n",
        "    self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
        "    self.state = state\n",
        "    self.isEnd = False\n",
        "\n",
        "  def giveReward(self):\n",
        "    \"\"\"\n",
        "    There are 3 obstacles in the 2d grid, and if any of the state falls under\n",
        "    the below mentioned category they get a reward\n",
        "    :return: Returns a reward value if it is equal to the any of the mentioned category\n",
        "    :rtype: int, this returns only one of the following value 1, -1 and -5   \n",
        "    \"\"\"\n",
        "    if self.state == WIN_STATE:\n",
        "        return 1\n",
        "    elif self.state == Zombie:\n",
        "        return -1\n",
        "    elif self.state == Wolf:\n",
        "        return -5\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "  def isEndFunc(self):\n",
        "    \"\"\"\n",
        "    This function is used to stop the reinforcement learning for the current round\n",
        "    if it falls under any of the following states WIN_STATE, zombie and wolf\n",
        "    :return: Returns a boolean value \"True\"\n",
        "    :rtype: bool\n",
        "    \"\"\"\n",
        "    if (self.state == WIN_STATE) or (self.state == Zombie) or (self.state == Wolf):\n",
        "      self.isEnd = True\n",
        "\n",
        "  def nxtPosition(self, action):\n",
        "    \"\"\"\n",
        "    Returns the next position state/cell value based on the agents action and if the next \n",
        "    state falls under in any of the teleport dicitonary keys the next state will be not up,\n",
        "    down, left and right rather it jumps multiple cells depending upon the dictionary value.\n",
        "    Similarly, there is a variabel called pull_back state, which pulls you back to the previous\n",
        "    state if your agent action resulted in any of the state present in the pull back list.\n",
        "\n",
        "    :param action: Contains any one of the following string value (up, down,left, right)\n",
        "    :param type: String\n",
        "    :return: Returns the agents tuple value if it not falls under any of the teleport dicitonary or\n",
        "    pull back method or else returns the agents desired next state.\n",
        "    :rtype: tuple \n",
        "    \"\"\"\n",
        "    teleport_dictionary={(4,2):(1,7),(1,2):(4,2),(3,4):(1,7)}\n",
        "    pull_back = [(5,2),(4,7)]\n",
        "    if action == \"up\":\n",
        "        nxtState = (self.state[0] - 1, self.state[1])\n",
        "    elif action == \"down\":\n",
        "        nxtState = (self.state[0] + 1, self.state[1])\n",
        "    elif action == \"left\":\n",
        "        nxtState = (self.state[0], self.state[1] - 1)\n",
        "    else:\n",
        "        nxtState = (self.state[0], self.state[1] + 1)\n",
        "\n",
        "    # if next state legal\n",
        "    if (nxtState[0] >= 0) and (nxtState[0] <= (BOARD_ROWS -1)):\n",
        "        if (nxtState[1] >= 0) and (nxtState[1] <= (BOARD_COLS -1)):\n",
        "          #Checks if the next state falls in teleport keys\n",
        "          if nxtState in teleport_dictionary.keys():\n",
        "            return teleport_dictionary[(nxtState[0],nxtState[1])]\n",
        "          # Checks if the next state is present in the pull back list\n",
        "          if nxtState in pull_back:\n",
        "            print(\"I'm in pull back\")\n",
        "\n",
        "            return (self.state[0], self.state[1])\n",
        "          return nxtState\n",
        "    return self.state"
      ],
      "metadata": {
        "id": "6eEflo9LmdGn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  \"\"\"\n",
        "  This class is responsible for choosing the action until it reaches the episode (Win or Lose). \n",
        "  This class depends upon the state class to move to the next state. Depending upon each episode\n",
        "  the agent learns and during the next play takes the best action.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Constructor Method\n",
        "    \"\"\" \n",
        "    self.states = []\n",
        "    self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
        "    self.State = State()\n",
        "    self.lr = 0.2\n",
        "    self.exp_rate = 0.3\n",
        "\n",
        "    # initial state reward\n",
        "    self.state_values = {}\n",
        "    for i in range(BOARD_ROWS):\n",
        "        for j in range(BOARD_COLS):\n",
        "            self.state_values[(i, j)] = 0\n",
        "\n",
        "  def chooseAction(self):\n",
        "    \"\"\"\n",
        "    The agent chooses the action to exploit all the cells and then choose the best possible\n",
        "    action or the agent greedy method can be controlled by a exploitation value called \n",
        "    exp_rate which is 0.2 everytime. \n",
        "\n",
        "    :return: Returns any one of the following value \"up\", \"down\", \"left\" or \"right\" \n",
        "    :rtype: String \n",
        "    \"\"\"\n",
        "    # choose action with most expected value\n",
        "    mx_nxt_reward = 0\n",
        "    action = \"\" \n",
        "\n",
        "    if np.random.uniform(0, 1) <= self.exp_rate:\n",
        "      action = np.random.choice(self.actions)\n",
        "    else:\n",
        "        # greedy action\n",
        "      for a in self.actions:\n",
        "          # if the action is deterministic\n",
        "        nxt_reward = self.state_values[self.State.nxtPosition(a)]\n",
        "        if nxt_reward >= mx_nxt_reward:\n",
        "          action = a\n",
        "          mx_nxt_reward = nxt_reward\n",
        "    return action\n",
        "\n",
        "  def takeAction(self, action):\n",
        "    \"\"\"\n",
        "    The agent takes the neccessary action (move up,down,left or right) after the action has \n",
        "    been chosen from the Class Agent:chooseAction() method.\n",
        "\n",
        "    :param action: Contains any one of the following string value (up, down,left, right)\n",
        "    :param type: String\n",
        "\n",
        "    :return: Updates the state class with new position as the START value\n",
        "    :rtype: Returns the state object from (Class: STATE)\n",
        "    \"\"\"\n",
        "    position = self.State.nxtPosition(action)\n",
        "    return State(state=position)\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    This method resets all the state value and erases the state memory for the entire\n",
        "    game played\n",
        "    \"\"\"\n",
        "    self.states = []\n",
        "    self.State = State()\n",
        "\n",
        "  def play(self, rounds=10):\n",
        "    \"\"\"\n",
        "    This method is responsible for stopping the game if it reaches the episode and then by the \n",
        "    help of dictionary the agent backtracks and updates the reward value to all the previous states/cells \n",
        "    where it came from using a deterministic reinforcement learning formula called value iteration. By updating\n",
        "    the states using reward for each episdode will help the agent to reach its goal or the end state\n",
        "    withlin less steps when the game is played next time.\n",
        "\n",
        "    :param rounds: Total number of games to be played by the agent\n",
        "    :param type: int\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    while i < rounds:\n",
        "\n",
        "      if self.State.isEnd:\n",
        "        # back propagate\n",
        "        reward = self.State.giveReward()\n",
        "        # explicitly assign end state to reward values\n",
        "        self.state_values[self.State.state] = reward  # this is optional\n",
        "        print(\"Game End Reward\", reward)\n",
        "        for s in reversed(self.states):\n",
        "          reward = self.state_values[s] + self.lr * (reward - self.state_values[s])\n",
        "          self.state_values[s] = round(reward, 3)\n",
        "        self.reset()\n",
        "        i += 1\n",
        "      else:\n",
        "        action = self.chooseAction()\n",
        "        # append trace\n",
        "        self.states.append(self.State.nxtPosition(action))\n",
        "        print(\"current position {} action {}\".format(self.State.state, action))\n",
        "        # by taking the action, it reaches the next state\n",
        "        self.State = self.takeAction(action)\n",
        "        # mark is end\n",
        "        self.State.isEndFunc()\n",
        "        print(\"nxt state\", self.State.state)\n",
        "        print(\"---------------------\")\n",
        "  def showValues(self):\n",
        "    for i in range(0, BOARD_ROWS):\n",
        "      print('----------------------------------')\n",
        "      out = '| '\n",
        "      for j in range(0, BOARD_COLS):\n",
        "          out += str(self.state_values[(i, j)]).ljust(6) + ' | '\n",
        "      print(out)\n",
        "    print('----------------------------------')"
      ],
      "metadata": {
        "id": "gnt-Cx08nh23"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  \"\"\"\n",
        "  As you can see in the below matrix, the agent start from the position (row:7,column:0) and it reaches the end goal (row:1,column:7) where it gets a \n",
        "  reward of 1.0. The agent takes the maximum state values at each step or takes the random action to reduce the exploitation time when the agent starts\n",
        "  the game again to play.\n",
        "  \"\"\"\n",
        "    ag = Agent()\n",
        "    ag.play(50)\n",
        "    print(ag.showValues())"
      ],
      "metadata": {
        "id": "QQKf8OMq_XTv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}