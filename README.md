IMPORTANT NOTE: The filesize of some Temporal Difference files were too large for GitHub so we uploaded all of the Temporal Difference code here: https://drive.google.com/file/d/1PZehtn7VHEIGnd9IMxkRXXUUK5vmdQsm/view?usp=share_link

<h1>Bayesian Q Learning: A Novel Approach</h1>

<p>This repository contains the code for our research paper titled "Bayesian Q Learning: A Novel Approach". To run an experiment, follow the instructions below:</p>

<h2>Running an Experiment</h2>

<ol>
<li>Open the starter script <code>BayesianDeepQMetricsCustomCity35.py</code> located in the <code>experiments</code> directory.</li>

<li>Modify the hyperparameters based on your needs. In the experiments section of our research paper, we changed the following hyperparameters:
<ul>
<li>Environment</li>
<li>Level of noise</li>
<li>Reinforcement learning algorithm</li>
</ul>
</li>
</ol>

<h3>Changing the Environment</h3>

<p>To switch the environment in the starter script, modify the <code>build_asci_art()</code> function. For example, to change the environment from city to suburban, replace the entire function with the provided code below (which is the suburban environment code):</p>

<pre><code>
def build_asci_art(gridsize, rho):
        ########

        art = [[OBSTACLE_CHAR for i in range(50)] for j in range(50)]
        
        for r in range(0, 36):
          for c in range(23,29):
            art[r][c] = EMPTY_CHAR

        for r in range(8, 17):
          for c in range(11,37):
            art[r][c] = EMPTY_CHAR

        for r in range(3, 21):
          for c in range(11,15):
            art[r][c] = EMPTY_CHAR

        for r in range(2, 22):
          for c in range(33,38):
            art[r][c] = EMPTY_CHAR

        for r in range(2, 7):
          for c in range(33,49):
            art[r][c] = EMPTY_CHAR

        for r in range(7, 22):
          for c in range(45,49):
            art[r][c] = EMPTY_CHAR

        for r in range(16, 22):
          for c in range(38,45):
            art[r][c] = EMPTY_CHAR

        for r in range(3, 7):
          for c in range(0,15):
            art[r][c] = EMPTY_CHAR

        for r in range(7, 21):
          for c in range(0,4):
            art[r][c] = EMPTY_CHAR

        for r in range(16, 21):
          for c in range(4,11):
            art[r][c] = EMPTY_CHAR

        for r in range(30, 36):
          for c in range(9,23):
            art[r][c] = EMPTY_CHAR

        for r in range(36, 49):
          for c in range(8,13):
            art[r][c] = EMPTY_CHAR

        for r in range(36, 46):
          for c in range(16,22):
            art[r][c] = EMPTY_CHAR

        for r in range(39, 46):
          for c in range(22,26):
            art[r][c] = EMPTY_CHAR

        for r in range(37, 41):
          for c in range(26,48):
            art[r][c] = EMPTY_CHAR

        for r in range(41, 48):
          for c in range(26,32):
            art[r][c] = EMPTY_CHAR

        for r in range(43, 48):
          for c in range(32,48):
            art[r][c] = EMPTY_CHAR

        for r in range(41, 43):
          for c in range(44,48):
            art[r][c] = EMPTY_CHAR


        #green

        art[0][24] = GOAL_CHAR

        #red
        art[46][10] = PLAYER_CHAR

        art1 = []

        for r in art:
            art1.append(''.join(r))

        split_art = []
        for r in art1:
            for ch in r:
                split_art.append(ch)
        return np.array(split_art)
</code></pre>

<h3>Changing the Level of Noise</h3>

<p>To adjust the level of noise, change the line that calculates the reward. For instance, to increase the noise level, replace the following line:</p>

<pre><code>
reward = self.reward_matrix[self.state[0]][self.state[1]] + (random.randint(-15, 15)/100)
</code></pre>

<p>with:</p>

<pre><code>
reward = self.reward_matrix[self.state[0]][self.state[1]] + (random.randint(-35, 35)/100)
</code></pre>

<h3>Changing the Reinforcement Learning Algorithm</h3>

<p>To switch the reinforcement learning algorithm, browse the other scripts in the <code>experiments</code> directory for examples of different algorithms. Copy the relevant code for the algorithm you want to use and replace the current algorithm code in the starter script.</p>

<h2>Output</h2>

<p>Choose the name of the output file that will be generated by modifying the following line:</p>

<pre><code>
fileName = "15Epsilonsuburban.txt"
</code></pre>

<p>This will save the results of your experiment to the specified file.</p>

<h2>Credits</h2>

<p>The <code>BayesianQLearningAgent</code> class was written with the help of GPT-4.</p>


<h2>How to run Temporal Difference experiments:</h2>
<p>The repository contains code for research regarding the implementation of Temporal Differences in the research paper titled “Bayesian Q Learning: A Novel Approach”. To run an experiment, follow the instructions below:</p>
<ol>
<li>Open 2DGridDroneGoalForest_TD_April12.ipynb located in the experiments directory.</li>
<li>Update model hyper parameters as necessary. In the experiments segment of the research paper, the following hyper parameters were changed:
<ul>
<li>Change hyperparameters: gamma, alpha, episodes, alpha_init, beta1, beta2, epsilon_decay, etc.</li>
<li>Changing reinforcement learning algorithm</li>
</ul>
To adjust the hyper parameters, change the line that calculates the values. For instance, to increase the number of episodes you would replace the following line:
<pre><code>def temporal_difference_learning(env, agent, alpha = 0.1, gamma = 0.99, episodes = 100, annealing_eps = True, alpha_init = 0.1, beta1 = 0.9, beta2 = 0.999, eps=1e-8):</code></pre>
with:
<pre><code>def temporal_difference_learning(env, agent, alpha = 0.1, gamma = 0.99, episodes = 10000, annealing_eps = True, alpha_init = 0.1, beta1 = 0.9, beta2 = 0.999, eps=1e-8):</code></pre>
</li>
</ol>
<h2>Changing the Reinforcement Learning Algorithm:</h2>
<p>To implement a different reinforcement learning algorithm, look through the experiments for examples of different use cases for Temporal Differences. Copy the relevant code for the algorithm you want to use and replace as necessary.</p>
<ul>
<li>Developed an EpsilonGreedyAgent class, temporal_difference_learning algorithm, extract_policy and run_agent_with_policy functions to achieve this task. The code works as expected although there were some bugs that I later solved and later integrated with the previous code base.</li>
<li>I implemented a Temporal Differences with the agent in different environments to see performance</li>
<li>Incorporated adaptive learning rate (using RMSProp) and dynamic exploration rate (annealing epsilon) for the TD learning Algorithm to measure improvements</li>
<li>Built an EpsilonGreedyAgent while incorporating the adaptive learning rate and dynamic exploration rate for the Temporal Difference Learning Algorithm and compared those outputs to measure improved performance. The main goal of this was to help the exploration strategy because there is an exploration/exploitation tradeoff. Then I made use of the Q-values to extract the optimal policy for the agent.</li>
</ul>
<h2>Output:</h2>
<p>Choose the name of the output file that will be generated by modifying the following line:</p>
<pre><code>fileName = 'Results8_Apr12.txt'</code></pre>
<p>This will save the results of your experiment to the specified file.</p>
